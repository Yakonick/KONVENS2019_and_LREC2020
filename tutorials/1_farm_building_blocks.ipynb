{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FARM Building Blocks\n",
    "\n",
    "Welcome to the FARM building blocks tutorial! There are many different ways to make use of this repository, but in this notebook, we will be going through the most import building blocks that will help you harvest the rewards of a successfully trained NLP model.\n",
    "\n",
    "Happy FARMing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Text Classification\n",
    "\n",
    "GermEval 2018 (GermEval2018) (https://projects.fzai.h-da.de/iggsa/) is an open data set containing texts that need to be classified by whether they are offensive or not. There are a set of coarse and fine labels, but here we will only be looking at the coarse set which labels each example as either OFFENSE or OTHER. To tackle this task, we are going to build a classifier that is composed of Google's BERT language model and a feed forward neural network prediction head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is /Users/yasen/Desktop/учёба/Алиев/KONVENS2019_and_LREC2020\n"
     ]
    }
   ],
   "source": [
    "# Let's start by adjust the working directory so that it is the root of the repository\n",
    "# This should be run just once.\n",
    "\n",
    "import os\n",
    "os.chdir('../')\n",
    "print(\"Current working directory is {}\".format(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the imports we need\n",
    "\n",
    "import torch\n",
    "from farm.modeling.tokenization import BertTokenizer\n",
    "from farm.data_handler.processor import GermEval18CoarseProcessor, GermEval18FineProcessor, GermEval19ImplicitProcessor\n",
    "from farm.data_handler.data_silo import DataSilo\n",
    "from farm.modeling.language_model import Bert\n",
    "from farm.modeling.prediction_head import TextClassificationHead\n",
    "from farm.modeling.adaptive_model import AdaptiveModel\n",
    "from farm.experiment import initialize_optimizer\n",
    "from farm.train import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Devices available: cpu\n"
     ]
    }
   ],
   "source": [
    "# We need to fetch the right device to drive the growth of our model\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Devices available: {}\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/31/2021 16:34:26 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache, downloading to /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpqtx37252\n",
      "100%|██████████| 213450/213450 [09:15<00:00, 384.01B/s] \n",
      "03/31/2021 16:43:44 - INFO - pytorch_transformers.file_utils -   copying /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpqtx37252 to cache at /Users/yasen/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "03/31/2021 16:43:44 - INFO - pytorch_transformers.file_utils -   creating metadata file for /Users/yasen/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
      "03/31/2021 16:43:44 - INFO - pytorch_transformers.file_utils -   removing temp file /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpqtx37252\n",
      "03/31/2021 16:43:44 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /Users/yasen/.cache/torch/pytorch_transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
     ]
    }
   ],
   "source": [
    "# Here we initialize a tokenizer that will be used for preprocessing text\n",
    "# This is the BERT Tokenizer which uses the byte pair encoding method.\n",
    "# It is currently loaded with a German model\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=\"bert-base-cased\",\n",
    "    do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Selbst',\n",
       " 'ein',\n",
       " 'bl',\n",
       " '##inde',\n",
       " '##s',\n",
       " 'Hu',\n",
       " '##hn',\n",
       " 'findet',\n",
       " 'mal',\n",
       " 'ein',\n",
       " 'Korn',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can test out how it will do on an example sentence\n",
    "\n",
    "EXAMPLE_SENTENCE = \"Selbst ein blindes Huhn findet mal ein Korn.\"\n",
    "tokenizer.tokenize(EXAMPLE_SENTENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to prepare the data for the model, we need a set of\n",
    "# functions to transform data files into PyTorch Datasets.\n",
    "# We group these together in Processor objects.\n",
    "# We will need a new Processor object for each new source of data.\n",
    "# The abstract class can be found in farm.data_handling.processor.Processor\n",
    "\n",
    "processor = GermEval19ImplicitProcessor(tokenizer=tokenizer,\n",
    "                                      max_seq_len=128,\n",
    "                                      data_dir=\"data/germeval18\",\n",
    "                                      train_filename=\"INSAB_training.csv\",\n",
    "                                      test_filename=\"INSAB_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/31/2021 18:41:10 - INFO - farm.data_handler.data_silo -   Loading train set from: data/germeval18/INSAB_training.csv\n",
      "03/31/2021 18:53:34 - INFO - farm.data_handler.processor -   *** Show 3 random examples ***\n",
      "03/31/2021 18:53:34 - INFO - farm.data_handler.processor -   *** Example ***\n",
      "ID: INSAB_training-59820-0\n",
      "Clear Text: \n",
      " \ttext: Maps  The Graphic Designer's Barnstar For the numerous maps you've created for many, many articles. (Couldn't find a map barnstar)   \n",
      " \tlabel: OTHER\n",
      "Tokenized: \n",
      " \t tokens: ['Map', '##s', 'The', 'Graphic', 'Designer', \"'\", 's', 'Bar', '##nst', '##ar', 'For', 'the', 'numerous', 'maps', 'you', \"'\", 've', 'created', 'for', 'many', ',', 'many', 'articles', '.', '(', 'Couldn', \"'\", 't', 'find', 'a', 'map', 'barn', '##star', ')']\n",
      " \toffsets: [0, 3, 6, 10, 18, 26, 27, 29, 32, 35, 38, 42, 46, 55, 60, 63, 64, 67, 75, 79, 83, 85, 90, 98, 100, 101, 107, 108, 110, 115, 117, 121, 125, 129]\n",
      " \tstart_of_word: [True, False, True, True, True, False, False, True, False, False, True, True, True, True, True, False, False, True, True, True, False, True, True, False, True, False, False, False, True, True, True, True, False, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 21824, 1116, 1109, 24318, 25174, 112, 188, 6523, 22399, 1813, 1370, 1103, 2567, 7415, 1128, 112, 1396, 1687, 1111, 1242, 117, 1242, 4237, 119, 113, 23320, 112, 189, 1525, 170, 4520, 9328, 10058, 114, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabel_ids: 3\n",
      "03/31/2021 18:53:34 - INFO - farm.data_handler.processor -   *** Example ***\n",
      "ID: INSAB_training-175378-0\n",
      "Clear Text: \n",
      " \ttext: :I would think there was enough cultural references to the word to expand beyond a dictionary definition. Like 'I feel like pwning newbs' and stuff?\n",
      " \tlabel: OTHER\n",
      "Tokenized: \n",
      " \t tokens: [':', 'I', 'would', 'think', 'there', 'was', 'enough', 'cultural', 'references', 'to', 'the', 'word', 'to', 'expand', 'beyond', 'a', 'dictionary', 'definition', '.', 'Like', \"'\", 'I', 'feel', 'like', 'p', '##wning', 'new', '##bs', \"'\", 'and', 'stuff', '?']\n",
      " \toffsets: [0, 1, 3, 9, 15, 21, 25, 32, 41, 52, 55, 59, 64, 67, 74, 81, 83, 94, 104, 106, 111, 112, 114, 119, 124, 125, 131, 134, 136, 138, 142, 147]\n",
      " \tstart_of_word: [True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, False, True, True, True, False, True, False, False, True, True, False]\n",
      "Features: \n",
      " \tinput_ids: [101, 131, 146, 1156, 1341, 1175, 1108, 1536, 3057, 7732, 1106, 1103, 1937, 1106, 7380, 2894, 170, 17085, 5754, 119, 2409, 112, 146, 1631, 1176, 185, 18469, 1207, 4832, 112, 1105, 4333, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabel_ids: 3\n",
      "03/31/2021 18:53:34 - INFO - farm.data_handler.processor -   *** Example ***\n",
      "ID: INSAB_training-124266-0\n",
      "Clear Text: \n",
      " \ttext: Consider this infinite time-off on wiki )  talk \n",
      " \tlabel: OTHER\n",
      "Tokenized: \n",
      " \t tokens: ['Consider', 'this', 'infinite', 'time', '-', 'off', 'on', 'w', '##iki', ')', 'talk']\n",
      " \toffsets: [0, 9, 14, 23, 27, 28, 32, 35, 36, 40, 43]\n",
      " \tstart_of_word: [True, True, True, True, False, False, True, True, False, True, True]\n",
      "Features: \n",
      " \tinput_ids: [101, 25515, 1142, 13157, 1159, 118, 1228, 1113, 192, 12635, 114, 2037, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tpadding_mask: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tsegment_ids: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      " \tlabel_ids: 3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'label_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b367d98aeb33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m data_silo = DataSilo(\n\u001b[1;32m     10\u001b[0m     \u001b[0mprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     batch_size=BATCH_SIZE)\n\u001b[0m",
      "\u001b[0;32m~/Desktop/учёба/Алиев/KONVENS2019_and_LREC2020/farm/data_handler/data_silo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, processor, batch_size, distributed)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/учёба/Алиев/KONVENS2019_and_LREC2020/farm/data_handler/data_silo.py\u001b[0m in \u001b[0;36m_load_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading train set from: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/учёба/Алиев/KONVENS2019_and_LREC2020/farm/data_handler/processor.py\u001b[0m in \u001b[0;36mdataset_from_file\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_featurize_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/учёба/Алиев/KONVENS2019_and_LREC2020/farm/data_handler/processor.py\u001b[0m in \u001b[0;36m_create_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbasket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mfeatures_flat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_features_to_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/учёба/Алиев/KONVENS2019_and_LREC2020/farm/data_handler/dataset.py\u001b[0m in \u001b[0;36mconvert_features_to_dataset\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         cur_tensor = torch.tensor(\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m         \u001b[0mall_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/учёба/Алиев/KONVENS2019_and_LREC2020/farm/data_handler/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         cur_tensor = torch.tensor(\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         )\n\u001b[1;32m     20\u001b[0m         \u001b[0mall_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'label_ids'"
     ]
    }
   ],
   "source": [
    "# We need a DataSilo in order to keep our train, dev and test sets separate.\n",
    "# The DataSilo will call the functions in the Processor to generate these sets.\n",
    "# From the DataSilo, we can fetch a PyTorch DataLoader object which will\n",
    "# be passed on to the model.\n",
    "# Here is a good place to define a batch size for the model\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "data_silo = DataSilo(\n",
    "    processor=processor,\n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In FARM, we make a strong distinction between the language model and prediction head so that you can mix and match different building blocks for your needs.\n",
    "\n",
    "For example, in the transfer learning paradigm, you might have the one language model that you will be using for both document classification and NER. Or you perhaps you have a pretrained language model which you would like to adapt to your domain, then use for a downstream task such as question answering. \n",
    "\n",
    "All this is possible within FARM and requires only the replacement of a few modular components, as we shall see below.\n",
    "\n",
    "Let's first have a look at how we might set up a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/31/2021 18:57:00 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json not found in cache, downloading to /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpcis6halz\n",
      "100%|██████████| 433/433 [00:00<00:00, 75068.56B/s]\n",
      "03/31/2021 18:57:01 - INFO - pytorch_transformers.file_utils -   copying /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpcis6halz to cache at /Users/yasen/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "03/31/2021 18:57:01 - INFO - pytorch_transformers.file_utils -   creating metadata file for /Users/yasen/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "03/31/2021 18:57:01 - INFO - pytorch_transformers.file_utils -   removing temp file /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpcis6halz\n",
      "03/31/2021 18:57:01 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-config.json from cache at /Users/yasen/.cache/torch/pytorch_transformers/b945b69218e98b3e2c95acf911789741307dec43c698d35fad11c1ae28bda352.9da767be51e1327499df13488672789394e2ca38b877837e52618a67d7002391\n",
      "03/31/2021 18:57:01 - INFO - pytorch_transformers.modeling_utils -   Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "03/31/2021 18:57:03 - INFO - pytorch_transformers.file_utils -   https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin not found in cache, downloading to /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpdalvvmst\n",
      "100%|██████████| 435779157/435779157 [14:07<00:00, 514055.52B/s] \n",
      "03/31/2021 19:11:12 - INFO - pytorch_transformers.file_utils -   copying /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpdalvvmst to cache at /Users/yasen/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "03/31/2021 19:11:14 - INFO - pytorch_transformers.file_utils -   creating metadata file for /Users/yasen/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "03/31/2021 19:11:14 - INFO - pytorch_transformers.file_utils -   removing temp file /var/folders/r8/4cfgx5dn43vg6fdvc_1r345m0000gn/T/tmpdalvvmst\n",
      "03/31/2021 19:11:14 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-pytorch_model.bin from cache at /Users/yasen/.cache/torch/pytorch_transformers/35d8b9d36faaf46728a0192d82bf7d00137490cd6074e8500778afed552a67e5.3fadbea36527ae472139fe84cddaa65454d7429f12d543d80bfc3ad70de55ac2\n",
      "03/31/2021 19:11:19 - WARNING - farm.modeling.language_model -   Could not automatically detect from language model name what language it is. \n",
      "We guess it's an *ENGLISH* model ... \n",
      "If not: Init the language model by supplying the 'language' param.\n",
      "Example: Bert.load('my_mysterious_model_name', language='de')\n"
     ]
    }
   ],
   "source": [
    "# The language model is the foundation on which modern NLP systems are built.\n",
    "# They encapsulate a general understanding of sentence semantics\n",
    "# and are not specific to any one task.\n",
    "\n",
    "# Here we are using Google's BERT model as implemented by HuggingFace. \n",
    "# The model being loaded is a German model that we trained. \n",
    "# You can also change the MODEL_NAME_OR_PATH to point to a BERT model that you\n",
    "# have saved or download one connected to the HuggingFace repository.\n",
    "# See farm.modeling.language_model.PRETRAINED_MODEL_ARCHIVE_MAP for a list of\n",
    "# available models\n",
    "\n",
    "MODEL_NAME_OR_PATH = \"bert-base-cased\"\n",
    "\n",
    "language_model = Bert.load(MODEL_NAME_OR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A prediction head is a model that processes the output of the language model\n",
    "# for a specific task.\n",
    "# Prediction heads will look different depending on whether you're doing text classification\n",
    "# Named Entity Recognition (NER), question answering or some other task.\n",
    "# They should generate logits over the available prediction classes and contain methods\n",
    "# to convert these logits to losses or predictions \n",
    "\n",
    "# Here we use TextClassificationHead which receives a single fixed length sentence vector\n",
    "# and processes it using a feed forward neural network. layer_dims is a list of dimensions:\n",
    "# [input_dims, hidden_1_dims, hidden_2_dims ..., output_dims]\n",
    "\n",
    "# Here by default we have a single layer network.\n",
    "# It takes in a vector of length 768 (the default size of BERT's output).\n",
    "# It outputs a vector of length 2 (the number of classes in the GermEval18 (coarse) dataset)\n",
    "\n",
    "LAYER_DIMS = [768, 4]\n",
    "\n",
    "prediction_head = TextClassificationHead(layer_dims=LAYER_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The language model and prediction head are coupled together in the Adaptive Model.\n",
    "# This class takes care of model saving and loading and also coordinates\n",
    "# cases where there is more than one prediction head.\n",
    "\n",
    "# EMBEDS_DROPOUT_PROB is the probability that an element of the output vector from the\n",
    "# language model will be set to zero.\n",
    "EMBEDS_DROPOUT_PROB = 0.1\n",
    "\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_sequence\"],\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize a Bert Adam optimizer that has a linear warmup and warmdown\n",
    "# Here you can set learning rate, the warmup proportion and number of epochs to train for\n",
    "\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "N_EPOCHS = 1\n",
    "\n",
    "optimizer, warmup_linear = initialize_optimizer(\n",
    "    model=model,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_proportion=WARMUP_PROPORTION,\n",
    "    n_examples=data_silo.n_samples(\"train\"),\n",
    "    batch_size=data_silo.batch_size,\n",
    "    n_epochs=N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop handled by this\n",
    "# It will also trigger evaluation during training using the dev data\n",
    "# and after training using the test data.\n",
    "\n",
    "# Set N_GPU to a positive value if CUDA is available\n",
    "N_GPU = 0\n",
    "\n",
    "trainer = Trainer(\n",
    "    optimizer=optimizer,\n",
    "    data_silo=data_silo,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    warmup_linear=warmup_linear,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03/27/2021 18:49:51 - INFO - farm.train -   ***** Running training *****\n",
      "Train epoch 1/1:   0%|          | 0/141 [00:00<?, ?it/s]/Users/yasen/Desktop/учёба/Алиев/KONVENS2019_and_LREC2020/farm/modeling/optimization.py:341: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1005.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n",
      "Train epoch 1/1:  71%|███████   | 100/141 [1:43:10<44:42, 65.42s/it]  \n",
      "Evaluating:   0%|          | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "Evaluating:   6%|▋         | 1/16 [00:11<02:48, 11.24s/it]\u001b[A\n",
      "Evaluating:  12%|█▎        | 2/16 [00:22<02:37, 11.23s/it]\u001b[A\n",
      "Evaluating:  19%|█▉        | 3/16 [00:33<02:25, 11.22s/it]\u001b[A\n",
      "Evaluating:  25%|██▌       | 4/16 [00:44<02:15, 11.25s/it]\u001b[A\n",
      "Evaluating:  31%|███▏      | 5/16 [00:56<02:05, 11.42s/it]\u001b[A\n",
      "Evaluating:  38%|███▊      | 6/16 [01:07<01:53, 11.33s/it]\u001b[A\n",
      "Evaluating:  44%|████▍     | 7/16 [01:18<01:41, 11.24s/it]\u001b[A\n",
      "Evaluating:  50%|█████     | 8/16 [01:37<01:47, 13.49s/it]\u001b[A\n",
      "Evaluating:  56%|█████▋    | 9/16 [01:51<01:34, 13.56s/it]\u001b[A\n",
      "Evaluating:  62%|██████▎   | 10/16 [02:03<01:18, 13.16s/it]\u001b[A\n",
      "Evaluating:  69%|██████▉   | 11/16 [02:15<01:03, 12.63s/it]\u001b[A\n",
      "Evaluating:  75%|███████▌  | 12/16 [02:26<00:49, 12.36s/it]\u001b[A\n",
      "Evaluating:  81%|████████▏ | 13/16 [02:38<00:36, 12.15s/it]\u001b[A\n",
      "Evaluating:  88%|████████▊ | 14/16 [02:50<00:24, 12.28s/it]\u001b[A\n",
      "Evaluating:  94%|█████████▍| 15/16 [03:04<00:12, 12.63s/it]\u001b[A\n",
      "Evaluating: 100%|██████████| 16/16 [03:12<00:00, 12.01s/it]\u001b[A03/27/2021 20:37:16 - INFO - farm.eval -   \n",
      "***** Evaluation Results on Val data after 100 steps *****\n",
      "03/27/2021 20:37:16 - INFO - farm.eval -   \n",
      " _________ Prediction Head 0 _________\n",
      "03/27/2021 20:37:16 - INFO - farm.eval -   loss: 0.4946438670158386\n",
      "03/27/2021 20:37:16 - INFO - farm.eval -   f1_macro: 0.7333266140039225\n",
      "03/27/2021 20:37:16 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     OFFENSE     0.5815    0.8049    0.6752       164\n",
      "       OTHER     0.8828    0.7173    0.7915       336\n",
      "\n",
      "    accuracy                         0.7460       500\n",
      "   macro avg     0.7321    0.7611    0.7333       500\n",
      "weighted avg     0.7840    0.7460    0.7533       500\n",
      "\n",
      "Train epoch 1/1: 100%|██████████| 141/141 [2:17:52<00:00, 42.16s/it]   \n",
      "Evaluating: 100%|██████████| 111/111 [22:44<00:00, 12.29s/it]\n",
      "03/27/2021 21:30:28 - INFO - farm.eval -   \n",
      "***** Evaluation Results on Test data after 141 steps *****\n",
      "03/27/2021 21:30:28 - INFO - farm.eval -   \n",
      " _________ Prediction Head 0 _________\n",
      "03/27/2021 21:30:28 - INFO - farm.eval -   loss: 0.47207576036453247\n",
      "03/27/2021 21:30:28 - INFO - farm.eval -   f1_macro: 0.7329774860367364\n",
      "03/27/2021 21:30:28 - INFO - farm.eval -   report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     OFFENSE     0.7305    0.5458    0.6248      1202\n",
      "       OTHER     0.7926    0.8961    0.8412      2329\n",
      "\n",
      "    accuracy                         0.7768      3531\n",
      "   macro avg     0.7616    0.7209    0.7330      3531\n",
      "weighted avg     0.7715    0.7768    0.7675      3531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = trainer.train(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch to NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a transfer learning paradigm, there is a core computation that is shared amongst all tasks. FARM's modular structure means that you can easily swap out different building blocks to make the same language model work for many different tasks.\n",
    "\n",
    "We can adapt the above text classification model to NER by simply switching out the processor and prediction head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the new building blocks\n",
    "\n",
    "from farm.data_handler.processor import CONLLProcessor\n",
    "from farm.modeling.prediction_head import TokenClassificationHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This processor will preprocess the data for the CoNLL03 NER task\n",
    "\n",
    "ner_processor = CONLLProcessor(tokenizer=tokenizer,\n",
    "                               max_seq_len=128,\n",
    "                               data_dir=\"data/conll03de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This prediction head is also a feed forward neural network but expects one\n",
    "# vector per token in the input sequence and will generate a set of logits\n",
    "# for each input\n",
    "\n",
    "LAYER_DIMS = [768, 13]\n",
    "\n",
    "ner_prediction_head = TokenClassificationHead(layer_dims=LAYER_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can integrate these new pieces with the rest using this code\n",
    "# It is pretty much the same structure as what we had above for text classification\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EMBEDS_DROPOUT_PROB = 0.1\n",
    "LEARNING_RATE = 2e-5\n",
    "WARMUP_PROPORTION = 0.1\n",
    "N_EPOCHS = 1\n",
    "N_GPU = 0\n",
    "\n",
    "data_silo = DataSilo(\n",
    "    processor=ner_processor,\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "model = AdaptiveModel(\n",
    "    language_model=language_model,\n",
    "    prediction_heads=[ner_prediction_head],\n",
    "    embeds_dropout_prob=EMBEDS_DROPOUT_PROB,\n",
    "    lm_output_types=[\"per_token\"],\n",
    "    device=device)\n",
    "\n",
    "optimizer, warmup_linear = initialize_optimizer(\n",
    "    model=model,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_proportion=WARMUP_PROPORTION,\n",
    "    n_examples=data_silo.n_samples(\"train\"),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    n_epochs=N_EPOCHS)\n",
    "\n",
    "trainer = Trainer(\n",
    "    optimizer=optimizer,\n",
    "    data_silo=data_silo,\n",
    "    epochs=N_EPOCHS,\n",
    "    n_gpu=N_GPU,\n",
    "    warmup_linear=warmup_linear,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.train(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
